{
  "hash": "36efd8cb620f4d6382538d5acbf82c97",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"06 Black-Box Models with LIME\"\nauthor: \"Maximilian Muza\"\nparams:\n  data_dir: \"../../data/\"\n  models_dir: \"../../models/\"\n---\n\n\nLoad the absolute path to the data directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_dir <- params$data_dir\nmodels_dir <- params$models_dir\n```\n:::\n\n\n# Libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(h2o)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(tidyquant)\nlibrary(lime)\nlibrary(tools)\n```\n:::\n\n\n# Data\n\nLoad data definitions\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_path <- file.path(data_dir, \"data_definitions.xlsx\")\ndefinitions_raw_tbl   <- read_excel(full_path, sheet = 1, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n```\n\n\n:::\n\n```{.r .cell-code}\ndefinitions_raw_tbl |> as_tibble() |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 35 × 2\n#>    ...1                    ...2             \n#>    <chr>                   <chr>            \n#>  1 Education               1 'Below College'\n#>  2 <NA>                    2 'College'      \n#>  3 <NA>                    3 'Bachelor'     \n#>  4 <NA>                    4 'Master'       \n#>  5 <NA>                    5 'Doctor'       \n#>  6 <NA>                    <NA>             \n#>  7 EnvironmentSatisfaction 1 'Low'          \n#>  8 <NA>                    2 'Medium'       \n#>  9 <NA>                    3 'High'         \n#> 10 <NA>                    4 'Very High'    \n#> # ℹ 25 more rows\n```\n\n\n:::\n:::\n\n\nLoad employee attrition\n\n::: {.cell}\n\n```{.r .cell-code}\nemployee_attrition_tbl <- read_csv(file.path(data_dir, \"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nemployee_attrition_tbl |> as_tibble() |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1,470 × 35\n#>      Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n#>    <dbl> <chr>     <chr>                 <dbl> <chr>                       <dbl>\n#>  1    41 Yes       Travel_Rarely          1102 Sales                           1\n#>  2    49 No        Travel_Frequently       279 Research & Deve…                8\n#>  3    37 Yes       Travel_Rarely          1373 Research & Deve…                2\n#>  4    33 No        Travel_Frequently      1392 Research & Deve…                3\n#>  5    27 No        Travel_Rarely           591 Research & Deve…                2\n#>  6    32 No        Travel_Frequently      1005 Research & Deve…                2\n#>  7    59 No        Travel_Rarely          1324 Research & Deve…                3\n#>  8    30 No        Travel_Rarely          1358 Research & Deve…               24\n#>  9    38 No        Travel_Frequently       216 Research & Deve…               23\n#> 10    36 No        Travel_Rarely          1299 Research & Deve…               27\n#> # ℹ 1,460 more rows\n#> # ℹ 29 more variables: Education <dbl>, EducationField <chr>,\n#> #   EmployeeCount <dbl>, EmployeeNumber <dbl>, EnvironmentSatisfaction <dbl>,\n#> #   Gender <chr>, HourlyRate <dbl>, JobInvolvement <dbl>, JobLevel <dbl>,\n#> #   JobRole <chr>, JobSatisfaction <dbl>, MaritalStatus <chr>,\n#> #   MonthlyIncome <dbl>, MonthlyRate <dbl>, NumCompaniesWorked <dbl>,\n#> #   Over18 <chr>, OverTime <chr>, PercentSalaryHike <dbl>, …\n```\n\n\n:::\n:::\n\n\nMake HR data readable\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_hr_data_readable <- function(data, definitions_tbl) {\n  \n  definitions_list <- definitions_tbl %>%\n    fill(...1, .direction = \"down\") %>%\n    filter(!is.na(...2)) %>%\n    separate(...2, into = c(\"key\", \"value\"), sep = \" '\", remove = TRUE) %>%\n    rename(column_name = ...1) %>%\n    mutate(key = as.numeric(key)) %>%\n    mutate(value = value %>% str_replace(pattern = \"'\", replacement = \"\")) %>%\n    split(.$column_name) %>%\n    map(~ select(., -column_name)) %>%\n    map(~ mutate(., value = as_factor(value))) \n  \n  for (i in seq_along(definitions_list)) {\n    list_name <- names(definitions_list)[i]\n    colnames(definitions_list[[i]]) <- c(list_name, paste0(list_name, \"_value\"))\n  }\n  \n  data_merged_tbl <- list(HR_Data = data) %>%\n    append(definitions_list, after = 1) %>%\n    reduce(left_join) %>%\n    select(-one_of(names(definitions_list))) %>%\n    set_names(str_replace_all(names(.), pattern = \"_value\", \n                              replacement = \"\")) %>%\n    select(sort(names(.))) %>%\n    mutate_if(is.character, as.factor) %>%\n    mutate(\n     \n    )\n  \n  return(data_merged_tbl)\n  \n}\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Joining with `by = join_by(Education)`\n#> Joining with `by = join_by(EnvironmentSatisfaction)`\n#> Joining with `by = join_by(JobInvolvement)`\n#> Joining with `by = join_by(JobSatisfaction)`\n#> Joining with `by = join_by(PerformanceRating)`\n#> Joining with `by = join_by(RelationshipSatisfaction)`\n#> Joining with `by = join_by(WorkLifeBalance)`\n```\n\n\n:::\n:::\n\n\nCreate training and testing dataset (Hold-out method)\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nsplit <- initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\ntrain_tbl <- training(split)\ntest_tbl  <- testing(split)\n```\n:::\n\n\n# ML Preprocessing Recipe\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_obj <- recipe(Attrition ~ ., data = train_tbl) %>%\n                step_zv(all_predictors()) %>%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n                prep()\n\nrecipe_obj %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Inputs\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Number of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> outcome:    1\n#> predictor: 34\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Training information\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Training data contained 1249 data points and no incomplete rows.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Operations\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> • Zero variance filter removed: EmployeeCount and Over18, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> • Variable mutation for: JobLevel and StockOptionLevel | Trained\n```\n\n\n:::\n\n```{.r .cell-code}\ntrain_tbl <- bake(recipe_obj, new_data = train_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_tbl)\n```\n:::\n\n\n# H2O Model\n\n::: {.cell}\n\n```{.r .cell-code}\nh2o.init(max_mem_size=\"5G\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 minutes 47 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.44.0.3 \n#>     H2O cluster version age:    5 months and 30 days \n#>     H2O cluster name:           H2O_started_from_R_MaximilianMuza_qlj325 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   6.39 GB \n#>     H2O cluster total cores:    16 \n#>     H2O cluster allowed cores:  16 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.4.0 (2024-04-24 ucrt)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (5 months and 30 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_leader <- h2o.loadModel(\"C:/Users/MaximilianMuza/Documents/Projects/ss24-bdml-MaximilianMuza/models/StackedEnsemble_BestOfFamily_2_AutoML_1_20240619_143112\")\nautoml_leader\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_2_AutoML_1_20240619_143112 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              3/3\n#> 3      # GBM base models (used / total)              1/1\n#> 4      # GLM base models (used / total)              1/1\n#> 5      # DRF base models (used / total)              1/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.07823066\n#> RMSE:  0.2796975\n#> LogLoss:  0.2743965\n#> Mean Per-Class Error:  0.1868938\n#> AUC:  0.890229\n#> AUCPR:  0.7536347\n#> Gini:  0.7804581\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error      Rate\n#> No     849  43 0.048206   =43/892\n#> Yes     56 116 0.325581   =56/172\n#> Totals 905 159 0.093045  =99/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.337345   0.700906 120\n#> 2                       max f2  0.263161   0.714286 152\n#> 3                 max f0point5  0.483912   0.774254  75\n#> 4                 max accuracy  0.443321   0.909774  85\n#> 5                max precision  0.923664   1.000000   0\n#> 6                   max recall  0.006283   1.000000 389\n#> 7              max specificity  0.923664   1.000000   0\n#> 8             max absolute_mcc  0.337345   0.646614 120\n#> 9   max min_per_class_accuracy  0.194257   0.813953 195\n#> 10 max mean_per_class_accuracy  0.263161   0.826794 152\n#> 11                     max tns  0.923664 892.000000   0\n#> 12                     max fns  0.923664 171.000000   0\n#> 13                     max fps  0.000754 892.000000 399\n#> 14                     max tps  0.006283 172.000000 389\n#> 15                     max tnr  0.923664   1.000000   0\n#> 16                     max fnr  0.923664   0.994186   0\n#> 17                     max fpr  0.000754   1.000000 399\n#> 18                     max tpr  0.006283   1.000000 389\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.07682191\n#> RMSE:  0.2771677\n#> LogLoss:  0.255089\n#> Mean Per-Class Error:  0.1522504\n#> AUC:  0.9172527\n#> AUCPR:  0.6805513\n#> Gini:  0.8345054\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error     Rate\n#> No     145  13 0.082278  =13/158\n#> Yes      6  21 0.222222    =6/27\n#> Totals 151  34 0.102703  =19/185\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.364892   0.688525  33\n#> 2                       max f2  0.305397   0.789474  43\n#> 3                 max f0point5  0.364892   0.644172  33\n#> 4                 max accuracy  0.364892   0.897297  33\n#> 5                max precision  0.928331   1.000000   0\n#> 6                   max recall  0.058914   1.000000 115\n#> 7              max specificity  0.928331   1.000000   0\n#> 8             max absolute_mcc  0.340155   0.635386  36\n#> 9   max min_per_class_accuracy  0.305397   0.873418  43\n#> 10 max mean_per_class_accuracy  0.305397   0.881153  43\n#> 11                     max tns  0.928331 158.000000   0\n#> 12                     max fns  0.928331  26.000000   0\n#> 13                     max fps  0.001694 158.000000 184\n#> 14                     max tps  0.058914  27.000000 115\n#> 15                     max tnr  0.928331   1.000000   0\n#> 16                     max fnr  0.928331   0.962963   0\n#> 17                     max fpr  0.001694   1.000000 184\n#> 18                     max tpr  0.058914   1.000000 115\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.09561189\n#> RMSE:  0.3092117\n#> LogLoss:  0.3302917\n#> Mean Per-Class Error:  0.2601288\n#> AUC:  0.8311379\n#> AUCPR:  0.6050609\n#> Gini:  0.6622758\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error       Rate\n#> No     848  44 0.049327    =44/892\n#> Yes     81  91 0.470930    =81/172\n#> Totals 929 135 0.117481  =125/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.378371   0.592834 106\n#> 2                       max f2  0.186678   0.644841 203\n#> 3                 max f0point5  0.380485   0.639368 104\n#> 4                 max accuracy  0.380485   0.882519 104\n#> 5                max precision  0.905703   1.000000   0\n#> 6                   max recall  0.002584   1.000000 397\n#> 7              max specificity  0.905703   1.000000   0\n#> 8             max absolute_mcc  0.378371   0.530616 106\n#> 9   max min_per_class_accuracy  0.180220   0.761628 208\n#> 10 max mean_per_class_accuracy  0.186678   0.771405 203\n#> 11                     max tns  0.905703 892.000000   0\n#> 12                     max fns  0.905703 171.000000   0\n#> 13                     max fps  0.001012 892.000000 399\n#> 14                     max tps  0.002584 172.000000 397\n#> 15                     max tnr  0.905703   1.000000   0\n#> 16                     max fnr  0.905703   0.994186   0\n#> 17                     max fpr  0.001012   1.000000 399\n#> 18                     max tpr  0.002584   1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy   0.882994 0.020516   0.885057   0.851163   0.902655   0.898734\n#> auc        0.832872 0.021704   0.856107   0.815268   0.850269   0.806082\n#> err        0.117006 0.020516   0.114943   0.148837   0.097345   0.101266\n#> err_count 24.800000 4.604346  20.000000  32.000000  22.000000  24.000000\n#> f0point5   0.638412 0.064718   0.598291   0.589744   0.744048   0.657143\n#>           cv_5_valid\n#> accuracy    0.877359\n#> auc         0.836634\n#> err         0.122642\n#> err_count  26.000000\n#> f0point5    0.602837\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.653292  0.075788   0.608696   0.589744   0.781250\n#> r2                  0.286808  0.041894   0.264491   0.249023   0.357675\n#> recall              0.589408  0.055303   0.560000   0.589744   0.625000\n#> residual_deviance 139.981250 23.218420 102.873400 160.974750 147.333650\n#> rmse                0.309737  0.013784   0.300821   0.333936   0.305883\n#> specificity         0.939156  0.019168   0.939597   0.909091   0.962366\n#>                   cv_4_valid cv_5_valid\n#> precision           0.657143   0.629630\n#> r2                  0.278713   0.284138\n#> recall              0.657143   0.515151\n#> residual_deviance 155.457640 133.266780\n#> rmse                0.301311   0.306734\n#> specificity         0.940594   0.944134\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions_tbl <- automl_leader %>% \n    h2o.predict(newdata = as.h2o(test_tbl)) %>%\n    as.tibble() %>%\n    bind_cols(\n        test_tbl %>%\n            select(Attrition, EmployeeNumber)\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions_tbl %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 221 × 5\n#>    predict    No     Yes Attrition EmployeeNumber\n#>    <fct>   <dbl>   <dbl> <fct>              <dbl>\n#>  1 Yes     0.427 0.573   Yes                    1\n#>  2 No      0.990 0.00987 No                     2\n#>  3 No      0.652 0.348   No                     7\n#>  4 Yes     0.600 0.400   No                    10\n#>  5 No      0.875 0.125   No                    15\n#>  6 No      0.959 0.0414  No                    18\n#>  7 Yes     0.211 0.789   Yes                   19\n#>  8 No      0.802 0.198   No                    22\n#>  9 No      0.998 0.00175 No                    32\n#> 10 No      0.972 0.0282  No                    35\n#> # ℹ 211 more rows\n```\n\n\n:::\n:::\n\n\n# Explainer Object\n\n::: {.cell}\n\n```{.r .cell-code}\nexplainer <- train_tbl %>%\n  select(-Attrition) %>%\n  lime(\n    model           = automl_leader,\n    bin_continuous  = TRUE,\n    n_bins          = 4,\n    quantile_bins   = TRUE\n  )\nexplainer\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $model\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_2_AutoML_1_20240619_143112 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              3/3\n#> 3      # GBM base models (used / total)              1/1\n#> 4      # GLM base models (used / total)              1/1\n#> 5      # DRF base models (used / total)              1/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.07823066\n#> RMSE:  0.2796975\n#> LogLoss:  0.2743965\n#> Mean Per-Class Error:  0.1868938\n#> AUC:  0.890229\n#> AUCPR:  0.7536347\n#> Gini:  0.7804581\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error      Rate\n#> No     849  43 0.048206   =43/892\n#> Yes     56 116 0.325581   =56/172\n#> Totals 905 159 0.093045  =99/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.337345   0.700906 120\n#> 2                       max f2  0.263161   0.714286 152\n#> 3                 max f0point5  0.483912   0.774254  75\n#> 4                 max accuracy  0.443321   0.909774  85\n#> 5                max precision  0.923664   1.000000   0\n#> 6                   max recall  0.006283   1.000000 389\n#> 7              max specificity  0.923664   1.000000   0\n#> 8             max absolute_mcc  0.337345   0.646614 120\n#> 9   max min_per_class_accuracy  0.194257   0.813953 195\n#> 10 max mean_per_class_accuracy  0.263161   0.826794 152\n#> 11                     max tns  0.923664 892.000000   0\n#> 12                     max fns  0.923664 171.000000   0\n#> 13                     max fps  0.000754 892.000000 399\n#> 14                     max tps  0.006283 172.000000 389\n#> 15                     max tnr  0.923664   1.000000   0\n#> 16                     max fnr  0.923664   0.994186   0\n#> 17                     max fpr  0.000754   1.000000 399\n#> 18                     max tpr  0.006283   1.000000 389\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.07682191\n#> RMSE:  0.2771677\n#> LogLoss:  0.255089\n#> Mean Per-Class Error:  0.1522504\n#> AUC:  0.9172527\n#> AUCPR:  0.6805513\n#> Gini:  0.8345054\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error     Rate\n#> No     145  13 0.082278  =13/158\n#> Yes      6  21 0.222222    =6/27\n#> Totals 151  34 0.102703  =19/185\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.364892   0.688525  33\n#> 2                       max f2  0.305397   0.789474  43\n#> 3                 max f0point5  0.364892   0.644172  33\n#> 4                 max accuracy  0.364892   0.897297  33\n#> 5                max precision  0.928331   1.000000   0\n#> 6                   max recall  0.058914   1.000000 115\n#> 7              max specificity  0.928331   1.000000   0\n#> 8             max absolute_mcc  0.340155   0.635386  36\n#> 9   max min_per_class_accuracy  0.305397   0.873418  43\n#> 10 max mean_per_class_accuracy  0.305397   0.881153  43\n#> 11                     max tns  0.928331 158.000000   0\n#> 12                     max fns  0.928331  26.000000   0\n#> 13                     max fps  0.001694 158.000000 184\n#> 14                     max tps  0.058914  27.000000 115\n#> 15                     max tnr  0.928331   1.000000   0\n#> 16                     max fnr  0.928331   0.962963   0\n#> 17                     max fpr  0.001694   1.000000 184\n#> 18                     max tpr  0.058914   1.000000 115\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.09561189\n#> RMSE:  0.3092117\n#> LogLoss:  0.3302917\n#> Mean Per-Class Error:  0.2601288\n#> AUC:  0.8311379\n#> AUCPR:  0.6050609\n#> Gini:  0.6622758\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error       Rate\n#> No     848  44 0.049327    =44/892\n#> Yes     81  91 0.470930    =81/172\n#> Totals 929 135 0.117481  =125/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.378371   0.592834 106\n#> 2                       max f2  0.186678   0.644841 203\n#> 3                 max f0point5  0.380485   0.639368 104\n#> 4                 max accuracy  0.380485   0.882519 104\n#> 5                max precision  0.905703   1.000000   0\n#> 6                   max recall  0.002584   1.000000 397\n#> 7              max specificity  0.905703   1.000000   0\n#> 8             max absolute_mcc  0.378371   0.530616 106\n#> 9   max min_per_class_accuracy  0.180220   0.761628 208\n#> 10 max mean_per_class_accuracy  0.186678   0.771405 203\n#> 11                     max tns  0.905703 892.000000   0\n#> 12                     max fns  0.905703 171.000000   0\n#> 13                     max fps  0.001012 892.000000 399\n#> 14                     max tps  0.002584 172.000000 397\n#> 15                     max tnr  0.905703   1.000000   0\n#> 16                     max fnr  0.905703   0.994186   0\n#> 17                     max fpr  0.001012   1.000000 399\n#> 18                     max tpr  0.002584   1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy   0.882994 0.020516   0.885057   0.851163   0.902655   0.898734\n#> auc        0.832872 0.021704   0.856107   0.815268   0.850269   0.806082\n#> err        0.117006 0.020516   0.114943   0.148837   0.097345   0.101266\n#> err_count 24.800000 4.604346  20.000000  32.000000  22.000000  24.000000\n#> f0point5   0.638412 0.064718   0.598291   0.589744   0.744048   0.657143\n#>           cv_5_valid\n#> accuracy    0.877359\n#> auc         0.836634\n#> err         0.122642\n#> err_count  26.000000\n#> f0point5    0.602837\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.653292  0.075788   0.608696   0.589744   0.781250\n#> r2                  0.286808  0.041894   0.264491   0.249023   0.357675\n#> recall              0.589408  0.055303   0.560000   0.589744   0.625000\n#> residual_deviance 139.981250 23.218420 102.873400 160.974750 147.333650\n#> rmse                0.309737  0.013784   0.300821   0.333936   0.305883\n#> specificity         0.939156  0.019168   0.939597   0.909091   0.962366\n#>                   cv_4_valid cv_5_valid\n#> precision           0.657143   0.629630\n#> r2                  0.278713   0.284138\n#> recall              0.657143   0.515151\n#> residual_deviance 155.457640 133.266780\n#> rmse                0.301311   0.306734\n#> specificity         0.940594   0.944134\n#> \n#> $preprocess\n#> function (x) \n#> x\n#> <bytecode: 0x000002230345ad10>\n#> <environment: 0x000002230344f180>\n#> \n#> $bin_continuous\n#> [1] TRUE\n#> \n#> $n_bins\n#> [1] 4\n#> \n#> $quantile_bins\n#> [1] TRUE\n#> \n#> $use_density\n#> [1] TRUE\n#> \n#> $feature_type\n#>                      Age           BusinessTravel                DailyRate \n#>                \"numeric\"                 \"factor\"                \"numeric\" \n#>               Department         DistanceFromHome                Education \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>           EducationField           EmployeeNumber  EnvironmentSatisfaction \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>                   Gender               HourlyRate           JobInvolvement \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>                 JobLevel                  JobRole          JobSatisfaction \n#>                 \"factor\"                 \"factor\"                 \"factor\" \n#>            MaritalStatus            MonthlyIncome              MonthlyRate \n#>                 \"factor\"                \"numeric\"                \"numeric\" \n#>       NumCompaniesWorked                 OverTime        PercentSalaryHike \n#>                \"numeric\"                 \"factor\"                \"numeric\" \n#>        PerformanceRating RelationshipSatisfaction         StockOptionLevel \n#>                 \"factor\"                 \"factor\"                 \"factor\" \n#>        TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance \n#>                \"numeric\"                \"numeric\"                 \"factor\" \n#>           YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion \n#>                \"numeric\"                \"numeric\"                \"numeric\" \n#>     YearsWithCurrManager \n#>                \"numeric\" \n#> \n#> $bin_cuts\n#> $bin_cuts$Age\n#>   0%  25%  50%  75% 100% \n#>   18   30   36   43   60 \n#> \n#> $bin_cuts$BusinessTravel\n#> NULL\n#> \n#> $bin_cuts$DailyRate\n#>   0%  25%  50%  75% 100% \n#>  102  469  799 1157 1499 \n#> \n#> $bin_cuts$Department\n#> NULL\n#> \n#> $bin_cuts$DistanceFromHome\n#>   0%  25%  50%  75% 100% \n#>    1    2    7   13   29 \n#> \n#> $bin_cuts$Education\n#> NULL\n#> \n#> $bin_cuts$EducationField\n#> NULL\n#> \n#> $bin_cuts$EmployeeNumber\n#>   0%  25%  50%  75% 100% \n#>    4  496 1016 1550 2068 \n#> \n#> $bin_cuts$EnvironmentSatisfaction\n#> NULL\n#> \n#> $bin_cuts$Gender\n#> NULL\n#> \n#> $bin_cuts$HourlyRate\n#>   0%  25%  50%  75% 100% \n#>   30   48   66   83  100 \n#> \n#> $bin_cuts$JobInvolvement\n#> NULL\n#> \n#> $bin_cuts$JobLevel\n#> NULL\n#> \n#> $bin_cuts$JobRole\n#> NULL\n#> \n#> $bin_cuts$JobSatisfaction\n#> NULL\n#> \n#> $bin_cuts$MaritalStatus\n#> NULL\n#> \n#> $bin_cuts$MonthlyIncome\n#>    0%   25%   50%   75%  100% \n#>  1009  2909  4941  8564 19999 \n#> \n#> $bin_cuts$MonthlyRate\n#>    0%   25%   50%   75%  100% \n#>  2094  8018 14363 20489 26999 \n#> \n#> $bin_cuts$NumCompaniesWorked\n#>   0%  25%  50%  75% 100% \n#>    0    1    2    4    9 \n#> \n#> $bin_cuts$OverTime\n#> NULL\n#> \n#> $bin_cuts$PercentSalaryHike\n#>   0%  25%  50%  75% 100% \n#>   11   12   14   18   25 \n#> \n#> $bin_cuts$PerformanceRating\n#> NULL\n#> \n#> $bin_cuts$RelationshipSatisfaction\n#> NULL\n#> \n#> $bin_cuts$StockOptionLevel\n#> NULL\n#> \n#> $bin_cuts$TotalWorkingYears\n#>   0%  25%  50%  75% 100% \n#>    0    6   10   15   40 \n#> \n#> $bin_cuts$TrainingTimesLastYear\n#>   0%  25%  50% 100% \n#>    0    2    3    6 \n#> \n#> $bin_cuts$WorkLifeBalance\n#> NULL\n#> \n#> $bin_cuts$YearsAtCompany\n#>   0%  25%  50%  75% 100% \n#>    0    3    5    9   40 \n#> \n#> $bin_cuts$YearsInCurrentRole\n#>   0%  25%  50%  75% 100% \n#>    0    2    3    7   18 \n#> \n#> $bin_cuts$YearsSinceLastPromotion\n#>   0%  50%  75% 100% \n#>    0    1    3   15 \n#> \n#> $bin_cuts$YearsWithCurrManager\n#>   0%  25%  50%  75% 100% \n#>    0    2    3    7   17 \n#> \n#> \n#> $feature_distribution\n#> $feature_distribution$Age\n#> \n#>         1         2         3         4 \n#> 0.2650120 0.2810248 0.2193755 0.2345877 \n#> \n#> $feature_distribution$BusinessTravel\n#> \n#>        Non-Travel Travel_Frequently     Travel_Rarely \n#>        0.09527622        0.19135308        0.71337070 \n#> \n#> $feature_distribution$DailyRate\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2514011 0.2481986 \n#> \n#> $feature_distribution$Department\n#> \n#>        Human Resources Research & Development                  Sales \n#>             0.04083267             0.65172138             0.30744596 \n#> \n#> $feature_distribution$DistanceFromHome\n#> \n#>         1         2         3         4 \n#> 0.2906325 0.2369896 0.2233787 0.2489992 \n#> \n#> $feature_distribution$Education\n#> \n#> Below College       College      Bachelor        Master        Doctor \n#>     0.1216974     0.1913531     0.3835068     0.2698159     0.0336269 \n#> \n#> $feature_distribution$EducationField\n#> \n#>  Human Resources    Life Sciences        Marketing          Medical \n#>       0.01441153       0.41313050       0.11128903       0.31305044 \n#>            Other Technical Degree \n#>       0.05604484       0.09207366 \n#> \n#> $feature_distribution$EmployeeNumber\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$EnvironmentSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1929544 0.1977582 0.3066453 0.3026421 \n#> \n#> $feature_distribution$Gender\n#> \n#>   Female     Male \n#> 0.407526 0.592474 \n#> \n#> $feature_distribution$HourlyRate\n#> \n#>         1         2         3         4 \n#> 0.2602082 0.2538030 0.2377902 0.2481986 \n#> \n#> $feature_distribution$JobInvolvement\n#> \n#>        Low     Medium       High  Very High \n#> 0.05444355 0.25620496 0.59247398 0.09687750 \n#> \n#> $feature_distribution$JobLevel\n#> \n#>          1          2          3          4          5 \n#> 0.37309848 0.35468375 0.15452362 0.07285829 0.04483587 \n#> \n#> $feature_distribution$JobRole\n#> \n#> Healthcare Representative           Human Resources     Laboratory Technician \n#>                0.09207366                0.03282626                0.16973579 \n#>                   Manager    Manufacturing Director         Research Director \n#>                0.07045637                0.10008006                0.05444355 \n#>        Research Scientist           Sales Executive      Sales Representative \n#>                0.20096077                0.22017614                0.05924740 \n#> \n#> $feature_distribution$JobSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1937550 0.1905524 0.3002402 0.3154524 \n#> \n#> $feature_distribution$MaritalStatus\n#> \n#>  Divorced   Married    Single \n#> 0.2201761 0.4555645 0.3242594 \n#> \n#> $feature_distribution$MonthlyIncome\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$MonthlyRate\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$NumCompaniesWorked\n#> \n#>         1         2         3         4 \n#> 0.4907926 0.1000801 0.2009608 0.2081665 \n#> \n#> $feature_distribution$OverTime\n#> \n#>        No       Yes \n#> 0.7157726 0.2842274 \n#> \n#> $feature_distribution$PercentSalaryHike\n#> \n#>         1         2         3         4 \n#> 0.2754203 0.2834267 0.2417934 0.1993595 \n#> \n#> $feature_distribution$PerformanceRating\n#> \n#>         Low        Good   Excellent Outstanding \n#>   0.0000000   0.0000000   0.8502802   0.1497198 \n#> \n#> $feature_distribution$RelationshipSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1937550 0.2089672 0.3114492 0.2858287 \n#> \n#> $feature_distribution$StockOptionLevel\n#> \n#>          0          1          2          3 \n#> 0.43074460 0.40592474 0.10568455 0.05764612 \n#> \n#> $feature_distribution$TotalWorkingYears\n#> \n#>         1         2         3         4 \n#> 0.3002402 0.3274620 0.1329063 0.2393915 \n#> \n#> $feature_distribution$TrainingTimesLastYear\n#> \n#>         1         2         3 \n#> 0.4539632 0.3266613 0.2193755 \n#> \n#> $feature_distribution$WorkLifeBalance\n#> \n#>        Bad       Good     Better       Best \n#> 0.05684548 0.23538831 0.60448359 0.10328263 \n#> \n#> $feature_distribution$YearsAtCompany\n#> \n#>         1         2         3         4 \n#> 0.3218575 0.2097678 0.2193755 0.2489992 \n#> \n#> $feature_distribution$YearsInCurrentRole\n#> \n#>          1          2          3          4 \n#> 0.45956765 0.09367494 0.27141713 0.17534027 \n#> \n#> $feature_distribution$YearsSinceLastPromotion\n#> \n#>         1         2         3 \n#> 0.6389111 0.1481185 0.2129704 \n#> \n#> $feature_distribution$YearsWithCurrManager\n#> \n#>         1         2         3         4 \n#> 0.4619696 0.0968775 0.2586069 0.1825460 \n#> \n#> \n#> attr(,\"class\")\n#> [1] \"data_frame_explainer\" \"explainer\"            \"list\"\n```\n\n\n:::\n:::\n\n\n# Single Explanation Object + Plot\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use only one item\ntest_tbl %>%\n  slice(1) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 1\n#> Columns: 32\n#> $ Age                      <dbl> 41\n#> $ BusinessTravel           <fct> Travel_Rarely\n#> $ DailyRate                <dbl> 1102\n#> $ Department               <fct> Sales\n#> $ DistanceFromHome         <dbl> 1\n#> $ Education                <fct> College\n#> $ EducationField           <fct> Life Sciences\n#> $ EmployeeNumber           <dbl> 1\n#> $ EnvironmentSatisfaction  <fct> Medium\n#> $ Gender                   <fct> Female\n#> $ HourlyRate               <dbl> 94\n#> $ JobInvolvement           <fct> High\n#> $ JobLevel                 <fct> 2\n#> $ JobRole                  <fct> Sales Executive\n#> $ JobSatisfaction          <fct> Very High\n#> $ MaritalStatus            <fct> Single\n#> $ MonthlyIncome            <dbl> 5993\n#> $ MonthlyRate              <dbl> 19479\n#> $ NumCompaniesWorked       <dbl> 8\n#> $ OverTime                 <fct> Yes\n#> $ PercentSalaryHike        <dbl> 11\n#> $ PerformanceRating        <fct> Excellent\n#> $ RelationshipSatisfaction <fct> Low\n#> $ StockOptionLevel         <fct> 0\n#> $ TotalWorkingYears        <dbl> 8\n#> $ TrainingTimesLastYear    <dbl> 0\n#> $ WorkLifeBalance          <fct> Bad\n#> $ YearsAtCompany           <dbl> 6\n#> $ YearsInCurrentRole       <dbl> 4\n#> $ YearsSinceLastPromotion  <dbl> 0\n#> $ YearsWithCurrManager     <dbl> 5\n#> $ Attrition                <fct> Yes\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create explanation object\nexplanation <- test_tbl %>%\n  slice(1) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    explainer = explainer,\n    # Because it is a binary classification model: 1\n    n_labels   = 1,\n    # number of features to be returned\n    n_features = 8,\n    # number of localized linear models\n    n_permutations = 5000,\n    # Let's start with 1\n    kernel_width   = 4\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Select relevant columns\nexplanation %>%\n  as.tibble() %>%\n  select(feature:prediction) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 8 × 6\n#>   feature    feature_value feature_weight feature_desc data         prediction  \n#>   <chr>              <dbl>          <dbl> <chr>        <list>       <list>      \n#> 1 OverTime               2         0.191  OverTime = … <named list> <named list>\n#> 2 JobLevel               2        -0.0914 JobLevel = 2 <named list> <named list>\n#> 3 NumCompan…             8         0.0898 4 < NumComp… <named list> <named list>\n#> 4 Relations…             1         0.0797 Relationshi… <named list> <named list>\n#> 5 WorkLifeB…             1         0.0597 WorkLifeBal… <named list> <named list>\n#> 6 Department             3         0.0683 Department … <named list> <named list>\n#> 7 StockOpti…             1         0.0582 StockOption… <named list> <named list>\n#> 8 YearsSinc…             0        -0.0567 YearsSinceL… <named list> <named list>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot features\nplot_features(explanation = explanation, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](06_black_box_models_files/figure-html/unnamed-chunk-10-1.png){width=1536}\n:::\n:::\n\n\n# Multiple Explanation Object + Plot\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create multiple explanation object\nexplanation <- test_tbl %>%\n  slice(1:20) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    explainer = explainer,\n    n_labels   = 1,\n    n_features = 8,\n    n_permutations = 5000,\n    kernel_width   = 0.5\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\nexplanation %>%\n  as.tibble() %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 160 × 13\n#>    model_type   case  label label_prob model_r2 model_intercept model_prediction\n#>    <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n#>  1 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  2 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  3 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  4 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  5 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  6 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  7 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  8 classificat… 1     Yes        0.573    0.343           0.113            0.515\n#>  9 classificat… 2     No         0.990    0.389           0.561            0.886\n#> 10 classificat… 2     No         0.990    0.389           0.561            0.886\n#> # ℹ 150 more rows\n#> # ℹ 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#> #   feature_desc <chr>, data <list>, prediction <list>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the features\n# It can be seen that this plot is quite big and has lots of information\nplot_features(explanation, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](06_black_box_models_files/figure-html/unnamed-chunk-11-1.png){width=1536}\n:::\n\n```{.r .cell-code}\n# Plot the explanations\n# Plot is more condensed than the previous one, and preferably used\nplot_explanations(explanation)\n```\n\n::: {.cell-output-display}\n![](06_black_box_models_files/figure-html/unnamed-chunk-11-2.png){width=1536}\n:::\n:::\n\n\n### Code plot features method\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a label formatter that removes underscores and make label title casing\ncustom_labeller <- function(labels, multi_line = TRUE, sep = ': ') {\n  gsub(\"_\", \" \", names(labels))\n  names(labels) <- tools::toTitleCase(\n    gsub(\"_\", \" \", names(labels))\n  )\n  label_both(labels, multi_line, sep)\n}\n\n# Here the feature plot method\ncustom_feature_plot <- function(explanation, ncol) {\n  \n  # Define color palette\n  colors = c(\n    \"Supports\" = \"blue\",\n    \"Contradicts\" = \"red\"\n  )\n  \n  # Define fill color column that specifies the color of the row in the plot\n  # based on its key\n  explanation <- explanation %>%\n    mutate(\n      fill_color = ifelse(feature_weight > 0, \"Supports\", \"Contradicts\")\n    )\n  \n  # Create description by combining case and label, and also\n  # append feature description\n  explanation$description <- with(explanation, factor(\n    paste0(case, '_', label, format(feature_desc)),\n    levels = paste0(case, '_', label, format(feature_desc))[order(abs(feature_weight))]\n  ))\n  \n  # Format model R-squared and label probability to two decimal places\n  explanation$explanation_fit <- format(explanation$model_r2, digits = 2)\n  explanation$probability <- format(explanation$label_prob, digits = 2)\n  \n  # Convert label to a factor and order based on label probability in decreasing order\n  explanation$label <- factor(explanation$label, \n                              levels = unique(explanation$label[order(explanation$label_prob, decreasing = TRUE)]))\n  \n  # Create plot\n  ggplot(explanation) +\n    \n    # Create subplots based on the ncol parameter\n    facet_wrap(\n      ~ case + label + probability + explanation_fit,\n      labeller = custom_labeller,\n      scales = 'free_y',\n      ncol = ncol\n    ) +\n    \n    # Create column plots (similar to bar plot) for each case with feture weight\n    # as x-value\n    # Feature description on y axis\n    # Color of bar is defined based on the respective fill_color value\n    geom_col(aes(\n      x = feature_weight,\n      y = reorder(feature_desc, abs(feature_weight)),\n      fill = fill_color\n    )) +\n    \n    # Define x,y axis and title\n    labs(\n      title = \"Feature Importance\",\n      x = \"Feature Weight\",\n      y = \"Feature Description\",\n      fill = \"Support vs Contradiction\"\n    ) +\n    theme_minimal() +\n    theme(\n      panel.background = element_rect(fill = \"white\", colour = \"white\"),\n      plot.background = element_rect(fill = \"white\", colour = \"white\")\n    )\n}\n\n# Plot for the multiple explanation object\ncustom_feature_plot(explanation = explanation, ncol=4)\n```\n\n::: {.cell-output-display}\n![](06_black_box_models_files/figure-html/unnamed-chunk-12-1.png){width=1536}\n:::\n:::\n\n\n### Code plot explanations method\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here the explanation plot method\ncustom_explanation_plot <- function(explanation) {\n  # Set custom margin value\n  custom_margin = 15\n  \n  # Create plot\n  plot <- ggplot(explanation, aes_(~case, ~feature_desc)) +\n    \n    # Create tiles in plot with feature weight \n    geom_tile(aes_(fill = ~feature_weight)) +\n    \n    # Define discrete x and y axis\n    scale_x_discrete('Case', expand = c(0, 0)) +\n    scale_y_discrete('Feature', expand = c(0, 0)) +\n    \n    # Create color gradient from red to blue based on the value of feature weight\n    scale_fill_gradient2('Feature Weight', low = 'red', mid = 'white', high = 'blue') +\n    \n    # Define custom styling such as colors, border, and margin\n    theme_minimal() +\n    theme(\n      panel.border = element_rect(fill = NA, colour = 'grey60', size = 1),\n      panel.grid = element_blank(),\n      panel.background = element_rect(fill = \"white\", colour = \"white\"),\n      plot.background = element_rect(fill = \"white\", colour = \"white\"),\n      plot.margin = margin(custom_margin, custom_margin, custom_margin, custom_margin),\n      legend.position = 'bottom',\n    )\n\n  # Create subplots if for both label yes and no\n  if (is.null(explanation$label)) {\n    plot\n  } else {\n    plot + facet_wrap(~label)\n  }\n\n}\n\n# Call custom explanation plot with current explanation object as parameter\ncustom_explanation_plot(explanation = explanation)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: `aes_()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06_black_box_models_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}